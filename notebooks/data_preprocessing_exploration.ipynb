{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Preprocessing for NLP and Memory Optimization\n",
    "\n",
    "This notebook explores various text preprocessing techniques for natural language processing (NLP) tasks, with a focus on memory optimization using generators and iterators. We'll examine:\n",
    "\n",
    "1. Different text preprocessing methods for classification\n",
    "2. Memory-efficient data loading techniques\n",
    "3. Performance benchmarking of different approaches\n",
    "4. Integration with the DVC pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import gc\n",
    "import json\n",
    "import psutil\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# NLP libraries\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Download required NLTK resources\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add project root to path\n",
    "sys.path.append('..')\n",
    "\n",
    "# Import custom modules\n",
    "from src.config import Config\n",
    "from src.data.data_loader import DataLoader\n",
    "from src.data.preprocessing import TextPreprocessor\n",
    "from src.utils.decorators import timing_decorator, memory_usage_decorator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Create Sample Dataset for Experimentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a larger sample text classification dataset\n",
    "sentences = [\n",
    "    \"This product is amazing! I love it.\",\n",
    "    \"Terrible service, would not recommend to anyone.\",\n",
    "    \"The quality is acceptable but the price is too high.\",\n",
    "    \"Best purchase I've made all year! Works perfectly.\",\n",
    "    \"Completely useless product. Waste of money.\",\n",
    "    \"Not bad, but not great either. Just average.\",\n",
    "    \"The customer service was very helpful and responsive.\",\n",
    "    \"Broke after two days. Very disappointed.\",\n",
    "    \"Exceeded my expectations in every way possible!\",\n",
    "    \"Slightly overpriced for what you get, but decent quality.\"\n",
    "]\n",
    "labels = ['positive', 'negative', 'neutral', 'positive', 'negative', \n",
    "          'neutral', 'positive', 'negative', 'positive', 'neutral']\n",
    "\n",
    "# Create a larger dataset by repeating and modifying slightly\n",
    "import random\n",
    "random.seed(42)\n",
    "\n",
    "# Words to randomly insert/replace for variation\n",
    "adjectives = ['good', 'great', 'excellent', 'terrible', 'awful', 'bad', 'decent', 'fine', 'poor', 'exceptional']\n",
    "adverbs = ['very', 'extremely', 'somewhat', 'quite', 'rather', 'incredibly', 'barely', 'hardly', 'really', 'truly']\n",
    "\n",
    "large_texts = []\n",
    "large_labels = []\n",
    "\n",
    "for _ in range(100):  # Create 1000 examples\n",
    "    idx = random.randint(0, len(sentences)-1)\n",
    "    text = sentences[idx]\n",
    "    label = labels[idx]\n",
    "    \n",
    "    # Add some variation\n",
    "    if random.random() > 0.7:\n",
    "        words = text.split()\n",
    "        if len(words) > 3:\n",
    "            pos = random.randint(0, len(words)-1)\n",
    "            if random.random() > 0.5:\n",
    "                words[pos] = random.choice(adjectives)\n",
    "            else:\n",
    "                words.insert(pos, random.choice(adverbs))\n",
    "            text = ' '.join(words)\n",
    "    \n",
    "    large_texts.append(text)\n",
    "    large_labels.append(label)\n",
    "\n",
    "# Create DataFrame\n",
    "df_large = pd.DataFrame({\n",
    "    'text': large_texts,\n",
    "    'label': large_labels\n",
    "})\n",
    "\n",
    "print(f\"Created dataset with {len(df_large)} examples\")\n",
    "df_large.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to CSV for further experimentation\n",
    "os.makedirs('../data/raw', exist_ok=True)\n",
    "csv_path = '../data/raw/sample_reviews.csv'\n",
    "df_large.to_csv(csv_path, index=False)\n",
    "print(f\"Saved to {csv_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Basic Text Preprocessing Functions\n",
    "\n",
    "Let's examine how different preprocessing methods affect text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_preprocessing_steps(text):\n",
    "    \"\"\"Display the effect of each preprocessing step.\"\"\"\n",
    "    print(f\"Original: '{text}'\")\n",
    "    \n",
    "    # Lowercase\n",
    "    lowercase = text.lower()\n",
    "    print(f\"Lowercase: '{lowercase}'\")\n",
    "    \n",
    "    # Remove punctuation\n",
    "    import string\n",
    "    no_punct = lowercase.translate(str.maketrans('', '', string.punctuation))\n",
    "    print(f\"No punctuation: '{no_punct}'\")\n",
    "    \n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(no_punct)\n",
    "    print(f\"Tokenized: {tokens}\")\n",
    "    \n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "    print(f\"No stopwords: {filtered_tokens}\")\n",
    "    \n",
    "    # Stemming\n",
    "    stemmer = PorterStemmer()\n",
    "    stemmed = [stemmer.stem(token) for token in filtered_tokens]\n",
    "    print(f\"Stemmed: {stemmed}\")\n",
    "    \n",
    "    # Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized = [lemmatizer.lemmatize(token) for token in filtered_tokens]\n",
    "    print(f\"Lemmatized: {lemmatized}\")\n",
    "    \n",
    "    return {\n",
    "        'original': text,\n",
    "        'lowercase': lowercase,\n",
    "        'no_punct': no_punct,\n",
    "        'tokens': tokens,\n",
    "        'filtered': filtered_tokens,\n",
    "        'stemmed': stemmed,\n",
    "        'lemmatized': lemmatized\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with a sample sentence\n",
    "sample_sentence = \"This product is amazing! I love it and would recommend it to everyone.\"\n",
    "processed = display_preprocessing_steps(sample_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Using the TextPreprocessor Class\n",
    "\n",
    "Now, let's use our custom TextPreprocessor class with different configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize preprocessor with different configurations\n",
    "preprocessor_basic = TextPreprocessor({\n",
    "    'lowercase': True,\n",
    "    'remove_punctuation': True,\n",
    "    'remove_numbers': False,\n",
    "    'remove_stopwords': False,\n",
    "    'stemming': False,\n",
    "    'lemmatization': False\n",
    "})\n",
    "\n",
    "preprocessor_stopwords = TextPreprocessor({\n",
    "    'lowercase': True,\n",
    "    'remove_punctuation': True,\n",
    "    'remove_numbers': False,\n",
    "    'remove_stopwords': True,\n",
    "    'stemming': False,\n",
    "    'lemmatization': False\n",
    "})\n",
    "\n",
    "preprocessor_stemming = TextPreprocessor({\n",
    "    'lowercase': True,\n",
    "    'remove_punctuation': True,\n",
    "    'remove_numbers': False,\n",
    "    'remove_stopwords': True,\n",
    "    'stemming': True,\n",
    "    'lemmatization': False\n",
    "})\n",
    "\n",
    "preprocessor_lemmatization = TextPreprocessor({\n",
    "    'lowercase': True,\n",
    "    'remove_punctuation': True,\n",
    "    'remove_numbers': False,\n",
    "    'remove_stopwords': True,\n",
    "    'stemming': False,\n",
    "    'lemmatization': True\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare preprocessing results\n",
    "test_sentences = [\n",
    "    \"This product is amazing! I love it.\",\n",
    "    \"Terrible service, would not recommend to anyone.\",\n",
    "    \"The system crashed 3 times during the demo on 04/25/2023.\"\n",
    "]\n",
    "\n",
    "for sentence in test_sentences:\n",
    "    print(f\"\\nOriginal: {sentence}\")\n",
    "    print(f\"Basic: {preprocessor_basic.preprocess_text(sentence)}\")\n",
    "    print(f\"No Stopwords: {preprocessor_stopwords.preprocess_text(sentence)}\")\n",
    "    print(f\"Stemming: {preprocessor_stemming.preprocess_text(sentence)}\")\n",
    "    print(f\"Lemmatization: {preprocessor_lemmatization.preprocess_text(sentence)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Memory-Efficient Data Loading\n",
    "\n",
    "Now, let's explore how our memory-efficient data loading works using generators and iterators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the data loader\n",
    "config = Config()\n",
    "loader = DataLoader(csv_path, config)\n",
    "\n",
    "# Get dataset statistics\n",
    "stats = loader.get_statistics()\n",
    "print(f\"Dataset statistics:\\n{json.dumps(stats, indent=2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Loading Data in Batches vs. All at Once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to measure memory usage\n",
    "def get_memory_usage():\n",
    "    \"\"\"Get current memory usage in MB.\"\"\"\n",
    "    process = psutil.Process()\n",
    "    return process.memory_info().rss / (1024 * 1024)\n",
    "\n",
    "# Function to benchmark data loading methods\n",
    "def benchmark_loading_methods(data_path, batch_sizes=[10, 50, 100, None]):\n",
    "    \"\"\"Benchmark different data loading methods.\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for batch_size in batch_sizes:\n",
    "        # Create data loader with specified batch size\n",
    "        config = Config()\n",
    "        if batch_size:\n",
    "            config.batch_size = batch_size\n",
    "        loader = DataLoader(data_path, config)\n",
    "        \n",
    "        # Measure memory before\n",
    "        gc.collect()\n",
    "        time.sleep(1)  # Allow memory to stabilize\n",
    "        memory_before = get_memory_usage()\n",
    "        \n",
    "        # Start timing\n",
    "        start_time = time.time()\n",
    "        \n",
    "        if batch_size is None:\n",
    "            # Load all data at once using pandas\n",
    "            data = loader.load_pandas()\n",
    "            method = \"All at once (Pandas)\"\n",
    "        else:\n",
    "            # Process data in batches using generator\n",
    "            record_count = 0\n",
    "            for batch in loader.load_batch_generator(batch_size=batch_size):\n",
    "                record_count += len(batch)\n",
    "                # Simulate processing each record\n",
    "                for record in batch:\n",
    "                    _ = record['text'].lower()\n",
    "            method = f\"Batch size {batch_size}\"\n",
    "        \n",
    "        # End timing\n",
    "        elapsed_time = time.time() - start_time\n",
    "        \n",
    "        # Measure memory after\n",
    "        memory_after = get_memory_usage()\n",
    "        memory_increase = memory_after - memory_before\n",
    "        \n",
    "        results.append({\n",
    "            'method': method,\n",
    "            'time': elapsed_time,\n",
    "            'memory_increase_mb': memory_increase\n",
    "        })\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the benchmark\n",
    "benchmark_results = benchmark_loading_methods(csv_path)\n",
    "\n",
    "# Create a DataFrame from results\n",
    "df_benchmark = pd.DataFrame(benchmark_results)\n",
    "df_benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the benchmark results\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Plot execution time\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.barplot(x='method', y='time', data=df_benchmark)\n",
    "plt.title('Execution Time by Method')\n",
    "plt.xlabel('Method')\n",
    "plt.ylabel('Time (seconds)')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Plot memory usage\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.barplot(x='method', y='memory_increase_mb', data=df_benchmark)\n",
    "plt.title('Memory Usage by Method')\n",
    "plt.xlabel('Method')\n",
    "plt.ylabel('Memory Increase (MB)')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Iterating Through Records One by One"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the DataLoader's iterator interface\n",
    "loader = DataLoader(csv_path, config)\n",
    "\n",
    "# Process the first 5 records\n",
    "for i, record in enumerate(loader):\n",
    "    if i >= 5:  # Only show first 5\n",
    "        break\n",
    "    print(f\"Record {i+1}: {record['text'][:50]}... [{record['label']}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Benchmarking Preprocessing Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to benchmark preprocessing configurations\n",
    "def benchmark_preprocessing(data_path, configs):\n",
    "    \"\"\"Benchmark different preprocessing configurations.\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    # Load the sample data\n",
    "    df = pd.read_csv(data_path)\n",
    "    sample_texts = df['text'].tolist()[:20]  # Use first 20 for benchmark\n",
    "    \n",
    "    for config_name, config_options in configs.items():\n",
    "        # Create preprocessor\n",
    "        preprocessor = TextPreprocessor(config_options)\n",
    "        \n",
    "        # Start timing\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Process all sample texts\n",
    "        processed_texts = []\n",
    "        for text in sample_texts:\n",
    "            processed = preprocessor.preprocess_text(text)\n",
    "            processed_texts.append(processed)\n",
    "        \n",
    "        # End timing\n",
    "        elapsed_time = time.time() - start_time\n",
    "        \n",
    "        # Calculate average length reduction\n",
    "        orig_lengths = [len(text) for text in sample_texts]\n",
    "        proc_lengths = [len(text) for text in processed_texts]\n",
    "        avg_reduction = 1 - (sum(proc_lengths) / sum(orig_lengths))\n",
    "        \n",
    "        results.append({\n",
    "            'config': config_name,\n",
    "            'time': elapsed_time,\n",
    "            'avg_length_reduction': avg_reduction,\n",
    "            'sample_result': processed_texts[0]\n",
    "        })\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define preprocessing configurations to test\n",
    "preprocessing_configs = {\n",
    "    'Basic (lowercase only)': {\n",
    "        'lowercase': True,\n",
    "        'remove_punctuation': False,\n",
    "        'remove_stopwords': False,\n",
    "        'stemming': False,\n",
    "        'lemmatization': False\n",
    "    },\n",
    "    'No punctuation': {\n",
    "        'lowercase': True,\n",
    "        'remove_punctuation': True,\n",
    "        'remove_stopwords': False,\n",
    "        'stemming': False,\n",
    "        'lemmatization': False\n",
    "    },\n",
    "    'No stopwords': {\n",
    "        'lowercase': True,\n",
    "        'remove_punctuation': True,\n",
    "        'remove_stopwords': True,\n",
    "        'stemming': False,\n",
    "        'lemmatization': False\n",
    "    },\n",
    "    'Stemming': {\n",
    "        'lowercase': True,\n",
    "        'remove_punctuation': True,\n",
    "        'remove_stopwords': True,\n",
    "        'stemming': True,\n",
    "        'lemmatization': False\n",
    "    },\n",
    "    'Lemmatization': {\n",
    "        'lowercase': True,\n",
    "        'remove_punctuation': True,\n",
    "        'remove_stopwords': True,\n",
    "        'stemming': False,\n",
    "        'lemmatization': True\n",
    "    },\n",
    "    'Complete (all steps)': {\n",
    "        'lowercase': True,\n",
    "        'remove_punctuation': True,\n",
    "        'remove_numbers': True,\n",
    "        'remove_stopwords': True,\n",
    "        'stemming': True,\n",
    "        'lemmatization': True\n",
    "    }\n",
    "}\n",
    "\n",
    "# Run the benchmark\n",
    "preprocessing_results = benchmark_preprocessing(csv_path, preprocessing_configs)\n",
    "\n",
    "# Create a DataFrame from results\n",
    "df_preproc = pd.DataFrame(preprocessing_results)\n",
    "df_preproc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the preprocessing benchmark results\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Plot execution time\n",
    "plt.subplot(2, 1, 1)\n",
    "sns.barplot(x='config', y='time', data=df_preproc)\n",
    "plt.title('Preprocessing Time by Configuration')\n",
    "plt.xlabel('Configuration')\n",
    "plt.ylabel('Time (seconds)')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Plot length reduction\n",
    "plt.subplot(2, 1, 2)\n",
    "sns.barplot(x='config', y='avg_length_reduction', data=df_preproc)\n",
    "plt.title('Average Text Length Reduction by Configuration')\n",
    "plt.xlabel('Configuration')\n",
    "plt.ylabel('Length Reduction (percentage)')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. End-to-End Preprocessing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the data loader\n",
    "config = Config()\n",
    "loader = DataLoader(csv_path, config)\n",
    "\n",
    "# Set up the preprocessor with standard configuration\n",
    "preprocessor = TextPreprocessor({\n",
    "    'lowercase': True,\n",
    "    'remove_punctuation': True,\n",
    "    'remove_numbers': False,\n",
    "    'remove_stopwords': True,\n",
    "    'language': 'english',\n",
    "    'stemming': False,\n",
    "    'lemmatization': True\n",
    "})\n",
    "\n",
    "# Define output path\n",
    "output_path = os.path.join(config.processed_data_path, 'processed_sample_reviews.csv')\n",
    "\n",
    "# Process and save the dataset\n",
    "preprocessor.preprocess_and_save(loader, output_path)\n",
    "\n",
    "print(f\"Dataset processed and saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and examine the processed dataset\n",
    "df_processed = pd.read_csv(output_path)\n",
    "df_processed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare original and processed texts\n",
    "comparison = pd.DataFrame({\n",
    "    'original': df_large['text'].head(5),\n",
    "    'processed': df_processed['processed_text'].head(5)\n",
    "})\n",
    "comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Integration with DVC Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the DVC YAML pipeline configuration\n",
    "!cat ../dvc.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run DVC pipeline to preprocess data\n",
    "!cd .. && dvc run -n preprocess -d {csv_path} -d src/data/preprocessing.py -d src/data/data_loader.py -o {output_path} python -c \"from src.data.preprocessing import TextPreprocessor; from src.data.data_loader import DataLoader; from src.config import Config; config = Config(); preprocessor = TextPreprocessor(config.text_preprocessing); loader = DataLoader('{csv_path}', config); preprocessor.preprocess_and_save(loader, '{output_path}')\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Summary and Best Practices\n",
    "\n",
    "Based on our exploration, here are some best practices for text preprocessing and memory optimization:\n",
    "\n",
    "1. **Memory-Efficient Data Loading**:\n",
    "   - Use generators and iterators for large datasets\n",
    "   - Process data in batches rather than loading everything at once\n",
    "   - Choose an optimal batch size based on your memory constraints\n",
    "\n",
    "2. **Text Preprocessing**:\n",
    "   - Start with basic preprocessing (lowercase, remove punctuation)\n",
    "   - Remove stopwords for most classification tasks\n",
    "   - Choose between stemming (faster) and lemmatization (more accurate) based on your needs\n",
    "   - Customize preprocessing based on the specific domain\n",
    "\n",
    "3. **Pipeline Integration**:\n",
    "   - Use DVC to track both raw and processed datasets\n",
    "   - Ensure reproducibility by versioning preprocessing code along with data\n",
    "   - Create well-defined pipeline stages for preprocessing\n",
    "   - Track dependencies between data and code\n",
    "\n",
    "4. **Performance Considerations**:\n",
    "   - More complex preprocessing leads to more processing time\n",
    "   - Trade-offs between preprocessing thoroughness and speed/memory usage\n",
    "   - Consider using multiprocessing for very large datasets"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
