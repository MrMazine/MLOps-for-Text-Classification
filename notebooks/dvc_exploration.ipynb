{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DVC Exploration for Text Classification\n",
    "\n",
    "This notebook explores the use of Data Version Control (DVC) for text classification datasets. It demonstrates how to:\n",
    "\n",
    "1. Initialize a DVC repository\n",
    "2. Track and version datasets\n",
    "3. Create and switch between versions\n",
    "4. Work with remote storage\n",
    "5. Integrate with data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Installation\n",
    "\n",
    "First, let's make sure DVC is installed and we have the necessary dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install DVC if not already installed\n",
    "!pip install dvc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Add project root to path to import modules\n",
    "sys.path.append('..')\n",
    "\n",
    "# Import custom modules\n",
    "from src.config import Config\n",
    "from dvc_helpers.dvc_setup import DVCHandler\n",
    "from dvc_helpers.remote_storage import RemoteStorageManager"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Create a Sample Dataset\n",
    "\n",
    "Let's create a simple text classification dataset to use throughout this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sample text classification dataset\n",
    "data = {\n",
    "    'text': [\n",
    "        \"This movie was amazing! I loved every minute of it.\",\n",
    "        \"Worst film I've ever seen. Complete waste of time.\",\n",
    "        \"The acting was good but the plot was confusing.\",\n",
    "        \"Great performances by the entire cast. Highly recommended!\",\n",
    "        \"I fell asleep halfway through. Very boring.\",\n",
    "        \"Not bad, not great. Just an average film overall.\",\n",
    "        \"The special effects were incredible! Must see in 3D.\",\n",
    "        \"Terrible dialogue and poor character development.\",\n",
    "        \"One of the best films of the year. A true masterpiece.\",\n",
    "        \"I was disappointed by the ending. Expected more.\"\n",
    "    ],\n",
    "    'label': ['positive', 'negative', 'neutral', 'positive', 'negative', \n",
    "              'neutral', 'positive', 'negative', 'positive', 'negative']\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directories for data if they don't exist\n",
    "os.makedirs('../data/raw', exist_ok=True)\n",
    "os.makedirs('../data/processed', exist_ok=True)\n",
    "\n",
    "# Save to CSV\n",
    "csv_path = '../data/raw/movie_reviews_v1.csv'\n",
    "df.to_csv(csv_path, index=False)\n",
    "print(f\"Dataset saved to {csv_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Initialize and Configure DVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration\n",
    "config = Config()\n",
    "print(f\"Raw data path: {config.raw_data_path}\")\n",
    "print(f\"Processed data path: {config.processed_data_path}\")\n",
    "print(f\"DVC repo path: {config.dvc_repo_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize DVC handler\n",
    "dvc_handler = DVCHandler(config.dvc_repo_path)\n",
    "\n",
    "# Check if DVC is initialized\n",
    "if not dvc_handler.is_dvc_initialized():\n",
    "    print(\"Initializing DVC...\")\n",
    "    dvc_handler.initialize_dvc()\n",
    "else:\n",
    "    print(\"DVC is already initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get DVC information\n",
    "dvc_info = dvc_handler.get_dvc_info()\n",
    "print(\"DVC Information:\")\n",
    "print(f\"Initialized: {dvc_info['initialized']}\")\n",
    "print(f\"Repository Path: {dvc_info['repo_path']}\")\n",
    "print(f\"Remotes: {dvc_info['remotes']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Version Control the Dataset with DVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the dataset to DVC and commit\n",
    "dataset_path = csv_path\n",
    "success = dvc_handler.add_and_commit_dataset(dataset_path, \"Added initial movie reviews dataset (version 1)\")\n",
    "\n",
    "if success:\n",
    "    print(f\"Dataset {dataset_path} added to DVC and committed successfully\")\n",
    "else:\n",
    "    print(f\"Failed to add dataset {dataset_path} to DVC\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create and Track a Second Version of the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add more data to create a new version\n",
    "new_data = {\n",
    "    'text': [\n",
    "        \"The cinematography was breathtaking throughout the film.\",\n",
    "        \"Too many plot holes to be enjoyable.\",\n",
    "        \"I'm not sure what to think about this one.\",\n",
    "        \"A perfect example of modern storytelling.\",\n",
    "        \"The script was lazy and unimaginative.\"\n",
    "    ],\n",
    "    'label': ['positive', 'negative', 'neutral', 'positive', 'negative']\n",
    "}\n",
    "\n",
    "# Combine with the original dataset\n",
    "df_new = pd.DataFrame(new_data)\n",
    "df_combined = pd.concat([df, df_new], ignore_index=True)\n",
    "df_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the updated dataset\n",
    "csv_path_v2 = '../data/raw/movie_reviews_v2.csv'\n",
    "df_combined.to_csv(csv_path_v2, index=False)\n",
    "print(f\"Updated dataset saved to {csv_path_v2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add and commit the new version\n",
    "success = dvc_handler.add_and_commit_dataset(csv_path_v2, \"Added more reviews (version 2)\")\n",
    "\n",
    "if success:\n",
    "    print(f\"Dataset {csv_path_v2} added to DVC and committed successfully\")\n",
    "else:\n",
    "    print(f\"Failed to add dataset {csv_path_v2} to DVC\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. List All Versioned Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all datasets tracked by DVC\n",
    "datasets = dvc_handler.list_datasets()\n",
    "\n",
    "print(f\"Found {len(datasets)} versioned datasets:\")\n",
    "for dataset in datasets:\n",
    "    print(f\"- {dataset['path']} ({dataset['size_mb']} MB, last updated: {dataset['last_date']})\")\n",
    "    print(f\"  Last commit: {dataset['last_message']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Get Version History for a Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all versions of a specific dataset\n",
    "dataset_path = csv_path  # Using first dataset\n",
    "versions = dvc_handler.get_dataset_versions(dataset_path)\n",
    "\n",
    "print(f\"Version history for dataset {os.path.basename(dataset_path)}:\")\n",
    "for version in versions:\n",
    "    print(f\"- {version['date']}: {version['message']}\")\n",
    "    print(f\"  Author: {version['author']}\")\n",
    "    print(f\"  Commit: {version['hash']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Checkout a Specific Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the commit hash of the first version\n",
    "if versions and len(versions) > 0:\n",
    "    first_version_hash = versions[-1]['hash']  # Last in the list is the oldest\n",
    "    \n",
    "    # Checkout the first version\n",
    "    success = dvc_handler.checkout_version(dataset_path, first_version_hash)\n",
    "    \n",
    "    if success:\n",
    "        print(f\"Successfully checked out version {first_version_hash} of dataset\")\n",
    "        \n",
    "        # Verify by loading the data\n",
    "        df_checkout = pd.read_csv(dataset_path)\n",
    "        print(f\"Loaded dataset has {len(df_checkout)} records\")\n",
    "        display(df_checkout.head())\n",
    "    else:\n",
    "        print(f\"Failed to checkout version {first_version_hash}\")\n",
    "else:\n",
    "    print(\"No versions found for the dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Set Up Remote Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For demonstration, we'll use a local directory as remote storage\n",
    "remote_dir = '../remote_storage'\n",
    "os.makedirs(remote_dir, exist_ok=True)\n",
    "\n",
    "# Add remote to DVC\n",
    "remote_url = f\"file://{os.path.abspath(remote_dir)}\"\n",
    "remote_name = \"local-remote\"\n",
    "\n",
    "# Update config with remote info\n",
    "config.dvc_remote_url = remote_url\n",
    "config.dvc_remote_name = remote_name\n",
    "\n",
    "# Setup remote storage manager\n",
    "remote_storage = RemoteStorageManager(config)\n",
    "result = remote_storage.setup_remote()\n",
    "\n",
    "if result:\n",
    "    print(f\"Remote storage '{remote_name}' set up successfully with URL: {remote_url}\")\n",
    "else:\n",
    "    print(\"Failed to set up remote storage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Push data to remote storage\n",
    "result = remote_storage.push_to_remote()\n",
    "\n",
    "if result['success']:\n",
    "    print(result['message'])\n",
    "else:\n",
    "    print(f\"Failed to push to remote: {result['message']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Using Memory-Efficient Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data loader\n",
    "from src.data.data_loader import DataLoader\n",
    "\n",
    "# Initialize data loader with the dataset\n",
    "loader = DataLoader(csv_path, config)\n",
    "\n",
    "# Load data in batches\n",
    "batch_size = 3\n",
    "print(f\"Loading data in batches of {batch_size}:\")\n",
    "\n",
    "for i, batch in enumerate(loader.load_batch_generator(batch_size=batch_size)):\n",
    "    print(f\"Batch {i+1}:\")\n",
    "    for record in batch:\n",
    "        print(f\"  - {record['text'][:50]}... [{record['label']}]\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Data Preprocessing and Versioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import preprocessor\n",
    "from src.data.preprocessing import TextPreprocessor\n",
    "\n",
    "# Initialize preprocessor with custom options\n",
    "preprocessor = TextPreprocessor({\n",
    "    'lowercase': True,\n",
    "    'remove_punctuation': True,\n",
    "    'remove_stopwords': True,\n",
    "    'stemming': False,\n",
    "    'lemmatization': True\n",
    "})\n",
    "\n",
    "# Process a sample text\n",
    "sample_text = \"This movie was AMAZING! I loved every minute of it.\"\n",
    "processed_text = preprocessor.preprocess_text(sample_text)\n",
    "\n",
    "print(f\"Original: {sample_text}\")\n",
    "print(f\"Processed: {processed_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the entire dataset and save\n",
    "processed_path = os.path.join(config.processed_data_path, 'processed_movie_reviews.csv')\n",
    "preprocessor.preprocess_and_save(loader, processed_path)\n",
    "\n",
    "print(f\"Processed dataset saved to {processed_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the processed dataset\n",
    "processed_df = pd.read_csv(processed_path)\n",
    "processed_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Version the processed dataset\n",
    "success = dvc_handler.add_and_commit_dataset(processed_path, \"Added processed movie reviews dataset\")\n",
    "\n",
    "if success:\n",
    "    print(f\"Processed dataset added to DVC and committed successfully\")\n",
    "else:\n",
    "    print(f\"Failed to add processed dataset to DVC\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Data Statistics and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get statistics about the dataset\n",
    "stats = loader.get_statistics()\n",
    "\n",
    "print(\"Dataset Statistics:\")\n",
    "print(f\"Number of records: {stats['record_count']}\")\n",
    "print(f\"Available fields: {stats['fields']}\")\n",
    "print(f\"Average text length: {stats['avg_text_length']:.2f} characters\")\n",
    "print(f\"Min text length: {stats['min_text_length']} characters\")\n",
    "print(f\"Max text length: {stats['max_text_length']} characters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data validator\n",
    "from src.data.data_validator import DataValidator\n",
    "\n",
    "# Initialize validator\n",
    "validator = DataValidator(min_text_length=10, require_fields=['text', 'label'])\n",
    "\n",
    "# Validate the dataset\n",
    "is_valid, message = validator.validate_dataset(csv_path)\n",
    "\n",
    "print(f\"Dataset validation result: {message}\")\n",
    "print(f\"Is valid: {is_valid}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Using DVC Pipeline Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the content of the dvc.yaml file\n",
    "!cat ../dvc.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run DVC pipeline to preprocess data\n",
    "!cd .. && dvc run -n preprocess -d {csv_path} -d src/data/preprocessing.py -d src/data/data_loader.py -o {processed_path} python -c \"from src.data.preprocessing import TextPreprocessor; from src.data.data_loader import DataLoader; from src.config import Config; config = Config(); preprocessor = TextPreprocessor(config.text_preprocessing); loader = DataLoader('{csv_path}', config); preprocessor.preprocess_and_save(loader, '{processed_path}')\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Summary and Conclusion\n",
    "\n",
    "In this notebook, we explored how to use DVC for versioning text classification datasets. We learned how to:\n",
    "\n",
    "1. Initialize a DVC repository\n",
    "2. Add and version datasets\n",
    "3. Track different versions of datasets\n",
    "4. Checkout specific versions\n",
    "5. Set up remote storage for collaboration and backup\n",
    "6. Use memory-efficient data loading with generators\n",
    "7. Preprocess text data and version the processed datasets\n",
    "8. Analyze dataset statistics and validate data quality\n",
    "9. Create DVC pipelines for reproducible data processing\n",
    "\n",
    "These techniques form the foundation of a robust MLOps pipeline for text classification, ensuring reproducibility, traceability, and efficient data handling."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
